#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright Â© 2012 Mozilla Corporation

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.

import os
import sys
import argparse
import time
import datetime
import multiprocessing
import socket
import platform
import sqlite3
import json
import pickle

import BuildGetter

##
##
##

is_win = platform.system() == "Windows"

##
## Utility
##

def parse_nightly_time(string):
  string = string.split('-')
  if (len(string) != 3):
    raise Exception("Could not parse %s as a YYYY-MM-DD date")
  return datetime.date(int(string[0]), int(string[1]), int(string[2]))

# Grab the first file (alphanumerically) from the batch folder,
# delete it and return its contents
def get_queued_job(dirname):
  batchfiles = os.listdir(dirname)
  if len(batchfiles):
    bname = os.path.join(dirname, sorted(batchfiles)[0])
    try:
      bfile = open(bname, 'r')
      bcmd = json.load(bfile)
    finally:
      if bfile: bfile.close()
      os.remove(bname)
    return bcmd
  return False

# Given a 'hook', which is a path to a python file,
# imports it as a module and returns the handle. A bit hacky.
def _get_hook(filename):
  hookname = os.path.basename(filename)
  # Strip .py and complain if it has other periods. (I said hacky!)
  if hookname[-3:].lower() == '.py':
    hookname = hookname[:-3]
  if hookname.find('.') != -1:
    raise Exception("Hook filename cannot contain periods (other than .py (it is imported as a module interally))")

  # Add dir containing hook to path temporarily
  sys.path.append(os.path.abspath(os.path.dirname(filename)))
  try:
    ret = __import__(hookname)
  finally:
    sys.path = sys.path[:-1]
  return ret

# BatchBuild wraps BuildGetter.build with various info, and provides
# a .(de)serialize for the status.json file output
class BatchBuild():
  def __init__(self, build, revision):
    # BuildGetter compilebuild object
    self.build = build
    # Revision, may be full hash or partial
    self.revision = revision
    # Build / task number. These are sequential, but reset to zero occasionally
    # no two *concurrently running* builds will share a number, so it can be
    # used for e.g. non-colliding VNC display #s
    self.num = None
    # The pool task associated with this build
    self.task = None
    # Textual note for this build, shows up in logs and serialized Build objects.
    # used by AWSY's /status/ page, for instance
    self.note = None
    # Place in a custom series
    self.series = None
    # Timestamp of when the build began testing
    self.started = None
    # unique identifier per session, totally unique unlike .num. TODO should
    # probably be given to __init__ instead of set manually later...
    self.uid = -1
    # Timestamp of when this build was 'finished' (failed or otherwise)
    self.finished = None
    # If true, retest the build even if its already queued. --hook scripts should
    # honor this in should_test as well
    self.force = None

  @staticmethod
  def deserialize(buildobj, args):
    if buildobj['type'] == 'compile':
      if args.get('logdir'):
        logfile = os.path.join(args.get('logdir'), "%s.build.log" % (buildobj['revision'],))
      else:
        logfile = None
      build = BuildGetter.CompileBuild(args.get('repo'), args.get('mozconfig'), args.get('objdir'), pull=True, commit=buildobj['revision'], log=logfile)
    elif buildobj['type'] == 'tinderbox':
      build = BuildGetter.TinderboxBuild(buildobj['timestamp'], buildobj['branch'])
    elif buildobj['type'] == 'nightly':
      build = BuildGetter.NightlyBuild(parse_nightly_time(buildobj['for']))
    elif buildobj['type'] == 'ftp':
      build = BuildGetter.FTPBuild(buildobj['path'])
    else:
      raise Exception("Unkown build type %s" % buildobj['type'])

    ret = BatchBuild(build, buildobj['revision'])
    ret.series = buildobj['series']
    ret.uid = buildobj['uid']
    ret.timestamp = buildobj['timestamp']
    ret.note = buildobj['note']
    ret.started = buildobj['started']
    ret.finished = buildobj['finished']
    ret.force = buildobj['force']

    return ret

  def serialize(self):
    ret = {
      'timestamp' : self.build.get_buildtime(),
      'revision' : self.revision,
      'note' : self.note,
      'started' : self.started,
      'finished' : self.finished,
      'force' : self.force,
      'uid' : self.uid,
      'series': self.series
    }

    if isinstance(self.build, BuildGetter.CompileBuild):
      ret['type'] = 'compile'
    elif isinstance(self.build, BuildGetter.FTPBuild):
      ret['type'] = 'ftp'
      ret['path'] = self.build._path
    elif isinstance(self.build, BuildGetter.TinderboxBuild):
      # When deserializing we need to look this up by it's tinderbox timestamp,
      # even if we use the push timestamp internally
      ret['timestamp'] = self.build.get_tinderbox_timestamp()
      ret['type'] = 'tinderbox'
      ret['branch'] = self.build.get_branch()
    elif isinstance(self.build, BuildGetter.NightlyBuild):
      # Date of nightly might not correspond to build timestamp
      ret['for'] = '%u-%u-%u' % (self.build._date.year, self.build._date.month, self.build._date.day)
      ret['type'] = 'nightly'
    else:
      raise Exception("Unknown build type %s" % (build,))
    return ret

# Work around multiprocessing.Pool() quirkiness. We can't give it
# BatchTest.test_build directly because that might not point to the same thing
# in the child process (members are mutable). we also can't give it build
# directly because the pool pickles it at a later date and causes thread issues
# (but just forcing it to pickle explicitly is fine as it would be pickled
# eventually either way)
def _pool_batchtest_build(build, args):
  return BatchTest.test_build(pickle.loads(build), args)

##
## BatchTest - a threaded test object. Given a list of builds, prepares them
##             and tests them in parallel. In 'batch' mode, processes sets of
##             arguments from a batch folder, and adds them to its queue, never
##             exiting. (daemon mode might be better?)
##             See BatchTestCLI for documentation on options
##

class BatchTest(object):
  def __init__(self, args, out=sys.stdout):
    # See BatchTestCLI for args documentation, for the time being
    self.args = args
    self.logfile = None
    self.out = out
    self.starttime = time.time()
    self.buildindex = 0
    self.pool = None
    self.processed = 0
    self.tick = 0
    self.builds = {
      'building' : None,
      'prepared': [],
      'running': [],
      'pending': [],
      'skipped': [],
      'completed': [],
      'failed': []
    }
    self.processedbatches = []
    self.pendingbatches = []

    if (self.args.get('hook')):
      sys.path.append(os.path.abspath(os.path.dirname(self.args.get('hook'))))
      self.hook = os.path.basename(self.args.get('hook'))
    else:
      self.hook = None

    self.builder = None
    self.builder_mode = None
    self.builder_batch = None
    self.manager = multiprocessing.Manager()
    self.builder_result = self.manager.dict({ 'result': 'not started', 'ret' : None })

  def stat(self, msg=""):
    msg = "%s :: %s\n" % (time.ctime(), msg)
    if self.out:
      self.out.write("[BatchTester] %s" % msg)
    if self.logfile:
      self.logfile.write(msg)
      self.logfile.flush()

  #
  # Resets worker pool
  def reset_pool(self):
    if self.pool:
      self.pool.close()
      self.pool.join()
    self.buildindex = 0
    self.pool = multiprocessing.Pool(processes=self.args['processes'], maxtasksperchild=1)

  #
  # Writes/updates the status file
  def write_status(self):
    statfile = self.args.get('status_file')
    if not statfile: return
    status = {
              'starttime' : self.starttime,
              'building': self.builds['building'].serialize() if self.builds['building'] else None,
              'batches' : self.processedbatches,
              'pendingbatches' : self.pendingbatches
            }
    for x in self.builds:
      if type(self.builds[x]) == list:
        status[x] = map(lambda y: y.serialize(), self.builds[x])

    tempfile = os.path.join(os.path.dirname(statfile), ".%s" % os.path.basename(statfile))
    sf = open(tempfile, 'w')
    json.dump(status, sf, indent=2)
    if is_win:
      os.remove(statfile) # Can't do atomic renames on windows
    os.rename(tempfile, statfile)
    sf.close()

  # Builds that are in the pending/running list already
  def build_is_queued(self, build):
    for x in ( self.builds['running'], self.builds['pending'], self.builds['prepared'], [ self.builds['building'] ]):
      for y in x:
        if y and y.revision == build.revision:
          return True
    return False
  # Given a set of arguments, lookup & add all specified builds to our queue.
  # This happens asyncrhonously, so not all builds may be queued immediately
  def add_batch(self, batchargs):
    self.pendingbatches.append({ 'args' : batchargs, 'note' : None, 'requested' : time.time(), 'uid': self.processed })
    self.processed += 1

  # Checks on the builder subprocess, getting its result, starting it if needed,
  # etc
  def check_builder(self):
    # Did it exit?
    if self.builder and not self.builder.is_alive():
      self.builder.join()
      self.builder = None

      # Finished a batch queue job
      if self.builder_mode == 'batch':
        if self.builder_result['result'] == 'success':
          queued = self.queue_builds(self.builder_result['ret'][0], prepend=self.builder_batch['args'].get('prioritize'))
          already_queued = len(self.builder_result['ret'][0]) - len(queued)
          self.queue_builds(self.builder_result['ret'][1], target='skipped', prepend=self.builder_batch['args'].get('prioritize'))
          self.builder_batch['note'] = "Queued %u builds, skipped %u" % (len(queued), already_queued + len(self.builder_result['ret'][1]))
        else:
          self.builder_batch['note'] = self.builder_result['ret']
        self.stat("Batch completed: %s (%s)" % (self.builder_batch['args'], self.builder_batch['note']))
        self.builder_batch = None

      # Finished a build job
      elif self.builder_mode == 'build':
        build = self.builds['building']
        self.stat("Test %u prepared" % (build.num,))
        self.builds['building'] = None
        if self.builder_result['result'] == 'success':
          self.builds['prepared'].append(self.builder_result['ret'])
        else:
          build.note = "Build setup failed - see log"
          build.finished = time.time()
          self.builds['failed'].append(build)
      self.builder_result['result'] = 'uninitialied'
      self.builder_result['ret'] = None
      self.builder_mode = None

    # Should it run?
    if not self.builder and len(self.pendingbatches):
      self.builder_mode = 'batch'
      self.builder_batch = self.pendingbatches.pop()
      self.stat("Handling batch %s" % (self.builder_batch,))
      self.builder_batch['processed'] = time.time()
      self.processedbatches.append(self.builder_batch)
      self.builder_batch['note'] = "Processing - Looking up builds"
      self.builder = multiprocessing.Process(target=self._process_batch, args=(self.args, self.builder_batch['args'], self.builder_result, self.hook))
      self.builder.start()
    elif not self.builder and self.builds['building']:
      self.builder_mode = 'build'
      self.stat("Starting build for %s :: %s" % (self.builds['building'].num, self.builds['building'].serialize()))
      self.builder = multiprocessing.Process(target=self.prepare_build, args=(self.builds['building'], self.builder_result))
      self.builder.start()

  @staticmethod
  def prepare_build(build, result):
    if build.build.prepare():
      result['result'] = 'success'
    else:
      result['result'] = 'failed'

    result['ret'] = build

  # Add builds to self.builds[target], giving them a uid. Redirect builds from
  # pending -> skipped if they're already queued
  def queue_builds(self, builds, target='pending', prepend=False):
    skip = []
    ready = []
    for x in builds:
      if not x.force and target == 'pending' and self.build_is_queued(x):
        x.finished = time.time()
        skip.append(x)
        x.note = "A build with this revision is already in queue"
      else:
        ready.append(x)
      x.uid = self.processed
      self.processed += 1
    if len(skip):
      self.builds['skipped'].extend(skip)
    if (prepend):
      self.builds[target] = ready + self.builds[target]
    else:
      self.builds[target].extend(ready)
    return ready

  #
  # Run loop
  #
  def run(self):
    if not self.args.get('repo'):
      raise Exception('--repo is required for resolving full commit IDs (even on non-compile builds)')

    statfile = self.args.get("status_file")

    if self.args.get('logdir'):
      self.logfile = open(os.path.join(self.args.get('logdir'), 'tester.log'), 'a')

    self.stat("Starting at %s with args \"%s\"" % (time.ctime(), sys.argv))

    self.reset_pool()

    batchmode = self.args.get('batch')
    if batchmode:
      if statfile and os.path.exists(statfile) and self.args.get('status_resume'):
        sf = open(statfile, 'r')
        ostat = json.load(sf)
        sf.close()
        # Try to recover builds in order they were going to be processed
        recover_builds = ostat['running']
        recover_builds.extend(ostat['prepared'])
        if ostat['building']: recover_builds.append(ostat['building'])
        recover_builds.extend(ostat['pending'])

        if len(recover_builds):
          # Create a dummy batch, process it on main thread, move it to completed.
          # this all happens before the helper thread starts so there are no other
          # batches to contend with
          self.add_batch("< Tester Restarted : Resuming any interrupted builds >")
          resumebatch = self.pendingbatches.pop()
          self.processedbatches.append(resumebatch)
          resumebatch['processed'] = time.time()
          self.write_status()
          self.queue_builds(map(lambda x: BatchBuild.deserialize(x, self.args), recover_builds))
          resumebatch['note'] = "Recovered %u builds (%u skipped)" % (len(self.builds['pending']), len(self.builds['skipped']))
    else:
      self.add_batch(self.args)

    while True:
      # Clean up finished builds
      for build in self.builds['running']:
        if not build.task.ready(): continue

        taskresult = build.task.get() if build.task.successful() else False
        if taskresult is True:
          self.stat("Test %u finished" % (build.num,))
          self.builds['completed'].append(build)
        else:
          self.stat("!! Test %u failed :: %s" % (build.num, taskresult))
          build.note = "Failed: %s" % (taskresult,)
          self.builds['failed'].append(build)
        build.finished = time.time()
        self.builds['running'].remove(build)
        build.build.cleanup()

      # Check on builder
      self.check_builder()

      # Read any pending jobs if we're in batchmode
      while batchmode:
        rcmd = None
        try:
          rcmd = get_queued_job(batchmode)
        except Exception, e:
          note = "Invalid batch file"
          self.stat(note)
          self.processedbatches.append({ 'args' : "<parse error>", 'note': note })
        if rcmd:
          self.add_batch(rcmd)
        else:
          break

      # Prepare pending builds, but not more than processes, as prepared builds
      # takeup space (hundreds of queued builds would fill /tmp with gigabytes
      # of things)
      if len(self.builds['pending']) \
          and not self.builds['building'] \
          and len(self.builds['prepared']) < self.args['processes']:
        build = self.builds['pending'][0]
        self.builds['building'] = build
        self.builds['pending'].remove(build)
        build.num = self.buildindex
        self.buildindex += 1

      # Start builds if pool is not filled
      while len(self.builds['prepared']) and len(self.builds['running']) < self.args['processes']:
        build = self.builds['prepared'][0]
        self.builds['prepared'].remove(build)
        build.started = time.time()
        self.stat("Moving test %u to running" % (build.num,))
        build.task = self.pool.apply_async(_pool_batchtest_build, [pickle.dumps(build), self.args])
        self.builds['running'].append(build)

      self.write_status()

      in_progress = len(self.builds['pending']) + len(self.builds['prepared']) + len(self.builds['running'])
      if not self.builder and not self.builds['building'] and in_progress == 0:
        # Out of things to do
        if batchmode and self.buildindex > 0:
          # In batchmode, reset the pool and restore buildindex to zero.
          # Buildindex is used for things like VNC display IDs, so we don't want
          # it to get too high.
          self.reset_pool()
          self.buildindex = 0
        elif not batchmode:
          self.stat("All tasks complete, exiting")
          break # Done
      # Wait a little and repeat loop
      time.sleep(1)
      self.tick += 1
      if self.tick % 120 == 0:
        # Remove items older than 1 day from these lists
        self.builds['completed'] = filter(lambda x: (x.finished + 60 * 60 * 24) > time.time(), self.builds['completed'])
        self.builds['failed'] = filter(lambda x: (x.finished + 60 * 60 * 24 * 3) > time.time(), self.builds['failed'])
        self.builds['skipped'] = filter(lambda x: (x.finished + 60 * 60 * 24) > time.time(), self.builds['skipped'])
        self.processedbatches = filter(lambda x: (x['processed'] + 60 * 60 * 24) > time.time(), self.processedbatches)
      time.sleep(1)

    self.stat("No more tasks, exiting")
    self.pool.close()
    self.pool.join()
    self.pool = None

  # Threaded call the builder is started on. Calls _process_batch_inner and
  # handles return results
  @staticmethod
  def _process_batch(globalargs, batchargs, returnproxy, hook):
    try:
      if hook:
        mod = _get_hook(globalargs.get('hook'))
      else:
        mod = None
      ret = BatchTest._process_batch_inner(globalargs, batchargs, mod)
    except Exception, e:
      ret = "An exception occured while processing batch -- %s: %s" % (type(e), e)

    if type(ret) == str:
      returnproxy['result'] = 'error'
    else:
      returnproxy['result'] = 'success'
    returnproxy['ret'] = ret

  #
  # Inner call for _process_batch
  @staticmethod
  def _process_batch_inner(globalargs, batchargs, hook):
    if not batchargs['firstbuild']:
      raise Exception("--firstbuild is required")

    if not globalargs.get('no_pull'):
      # Do a tip lookup to pull the repo so get_full_revision is up to date
      BuildGetter.get_hg_range(globalargs.get('repo'), '.', '.', True)

    mode = batchargs['mode']
    dorange = 'lastbuild' in batchargs and batchargs['lastbuild']
    builds = []
    # Queue builds
    if mode == 'nightly':
      startdate = parse_nightly_time(batchargs['firstbuild'])
      if dorange:
        enddate = parse_nightly_time(batchargs['lastbuild'])
        dates = range(startdate.toordinal(), enddate.toordinal() + 1)
      else:
        dates = [ startdate.toordinal() ]
      for x in dates:
        builds.append(BuildGetter.NightlyBuild(datetime.date.fromordinal(x)))
    elif mode == 'tinderbox':
      startdate = float(batchargs['firstbuild'])
      if dorange:
        enddate = float(batchargs['lastbuild'])
        tinderbuilds = BuildGetter.list_tinderbox_builds(startdate, enddate)
        for x in tinderbuilds:
          builds.append(BuildGetter.TinderboxBuild(x))
      else:
        builds.append(BuildGetter.TinderboxBuild(startdate))
    elif mode == 'ftp':
      builds.append(BuildGetter.FTPBuild(batchargs['firstbuild']))
    elif mode == 'compile':
      repo = batchargs.get('repo') if batchargs.get('repo') else globalargs.get('repo')
      objdir = batchargs.get('objdir') if batchargs.get('objdir') else globalargs.get('objdir')
      mozconfig = batchargs.get('mozconfig') if batchargs.get('mozconfig') else globalargs.get('mozconfig')
      if not repo or not mozconfig or not objdir:
        raise Exception("Build mode requires --repo, --mozconfig, and --objdir to be set")
      if dorange:
        lastbuild = batchargs['lastbuild']
      else:
        lastbuild = batchargs['firstbuild']
      for commit in BuildGetter.get_hg_range(repo, batchargs['firstbuild'], lastbuild, not globalargs.get("no_pull")):
        if globalargs.get('logdir'):
          logfile = os.path.join(globalargs.get('logdir'), "%s.build.log" % (commit,))
        else:
          logfile = None
        builds.append(BuildGetter.CompileBuild(repo, mozconfig, objdir, pull=False, commit=commit, log=logfile))
    else:
      raise Exception("Unknown mode %s" % mode)

    readybuilds = []
    skippedbuilds = []
    force = batchargs.get('force') if batchargs.get('force') else globalargs.get('force')
    for build in builds:
      rev = build.get_revision()

      build = BatchBuild(build, rev)
      build.force = force
      build.series = batchargs.get('series')
      if not build.build.get_valid():
        # Can happen with FTP builds we failed to lookup on ftp.m.o, or any
        # builds that arn't found in pushlog
        build.note = "Build is not found or missing from pushlog"
      elif hook and not hook.should_test(build, globalargs):
        if not build.note:
          build.note = "Build skipped by tester";
      else:
        readybuilds.append(build)
        continue

      build.finished = time.time()
      skippedbuilds.append(build)

    return [ readybuilds, skippedbuilds ]

  #
  # Build testing pool
  #
  @staticmethod
  def test_build(build, globalargs):
    mod = None
    ret = True
    if not globalargs.get('hook'):
      return "Cannot test builds without a --hook providing run_tests(Build)"

    try:
      mod = _get_hook(globalargs.get('hook'))
      mod.run_tests(build, globalargs)
    except (Exception, KeyboardInterrupt) as e:
      err = "%s :: %s" % (type(e), e)
      ret = err
    return ret

class BatchTestCLI(BatchTest):
  def __init__(self, args=sys.argv[1:]):
    self.parser = argparse.ArgumentParser(description='Run tests against one or more builds in parallel')
    self.parser.add_argument('--mode', help='nightly or tinderbox or compile')
    self.parser.add_argument('--batch', help='Batch mode -- given a folder name, treat each file within as containing a set of arguments to this script, deleting each file as it is processed.')
    self.parser.add_argument('--firstbuild', help='For nightly, the date (YYYY-MM-DD) of the first build to test. For tinderbox, the timestamp to start testing builds at. For build, the first revision to build.')
    self.parser.add_argument('--lastbuild', help='[optional] For nightly builds, the last date to test. For tinderbox, the timestamp to stop testing builds at. For build, the last revision to build If omitted, first_build is the only build tested.')
    self.parser.add_argument('-p', '--processes', help='Number of tests to run in parallel.', default=1, type=int)
    self.parser.add_argument('--hook', help='Name of a python file to import for each test. The test will call should_test(BatchBuild), run_tests(BatchBuild), and cli_hook(argparser) in this file.')
    self.parser.add_argument('--logdir', '-l', help="Directory to log progress to. Doesn't make sense for batched processes. Creates 'tester.log', 'buildname.test.log' and 'buildname.build.log' (for compile builds).")
    self.parser.add_argument('--repo', help="For build mode, the checked out FF repo to use")
    self.parser.add_argument('--mozconfig', help="For build mode, the mozconfig to use")
    self.parser.add_argument('--objdir', help="For build mode, the objdir provided mozconfig will create")
    self.parser.add_argument('--no-pull', action='store_true', help="For build mode, don't run a hg pull in the repo before messing with a commit")
    self.parser.add_argument('--status-file', help="A file to keep a json-dump of the currently running job status in. This file is mv'd into place to avoid read/write issues")
    self.parser.add_argument('--status-resume', action='store_true', help="Resume any jobs still present in the status file. Useful for interrupted sessions")
    self.parser.add_argument('--prioritize', action='store_true', help="For batch'd builds, insert at the beginning of the pending queue rather than the end")
    self.parser.add_argument('--force', action='store_true', help="Test/queue given builds even if they have already been tested or are already in queue")
    temp = vars(self.parser.parse_known_args(args)[0])
    if temp.get('hook'):
      mod = _get_hook(temp.get('hook'))
      mod.cli_hook(self.parser)

    args = vars(self.parser.parse_args(args))
    super(BatchTestCLI, self).__init__(args)

#
# Main
#

if __name__ == '__main__':
  cli = BatchTestCLI()
  cli.run()
